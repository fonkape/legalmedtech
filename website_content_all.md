# Website Content Summary
Generiert am: 04.02.2025

## Metadata
**Title:** Legal Engineering für Healthcare-KI-Compliance | Daniel Kleiboldt
**Description:** KI in der Klinik ist kein IT-Projekt, sondern eine Haftungsfrage. Ich baue die architektonischen Leitplanken für rechtssichere Healthcare-KI. Sovereign AI & Governance.

---

## Navbar
**Logo:** Daniel Kleiboldt LEGAL ENGINEERING
**Links:**
- ÜBER MICH
- VORGEHEN
- ANGEBOT
- ARTIKEL
**CTA:** Erstgespräch buchen

---

## Hero (Startseite)
**Headline:** KI in der Arztpraxis? Nur mit Human Oversight.
**Subheadline:** Ihre Software kann alles. Aber wer haftet, wenn die KI halluziniert?
**Text:** Ich kombiniere juristische Expertise mit AI Engineering – damit Ihre Praxis-KI nicht nur effizient, sondern rechtssicher ist. Von Anfang an.
**Credentials:** Diplom-Jurist • AI Engineer • EU AI Act, MDR, DSGVO
**Buttons:**
- Compliance-Quickcheck buchen
- Artikel lesen

---

## ComplianceGap (Sektion "Methodik")
**Headline:** Ihr KI-Anbieter verkauft Features. Ich verkaufe Rechtssicherheit.

**Box 1:**
- **Headline:** Die KI halluziniert im Arztbrief → Sie haften persönlich.
- **Text:** Ich baue technische Prüfprozesse, die Haftung ausschließen – nicht nur dokumentieren, sondern erzwingen.
- **Link:** Compliance-Audit buchen

**Box 2:**
- **Headline:** Ihr Vertrag läuft bis 2027 → Ihre Software wird ab 2026 illegal.
- **Text:** Ich auditiere Verträge auf AI Act-Konformität und erstelle eine Compliance-Roadmap, bevor die Frist abläuft.
- **Link:** Vertrag prüfen lassen

**Box 3:**
- **Headline:** US-Cloud = Risiko für § 203 StGB (Schweigepflicht).
- **Text:** Ich baue lokale KI-Systeme (Sovereign AI), die Ihre Praxis nie verlassen – absolute Datensouveränität.
- **Link:** Datensouveränität anfragen

---

## Services (Sektion "Angebot")
**Headline:** Drei Wege zur Rechtssicherheit. Wählen Sie Ihren.

**Karte 1: Compliance-Quickcheck**
- **Sub:** Für MVZs, Fachkliniken, die KI bereits nutzen
- **Text:** Sie nutzen bereits Doctolib, CGM oder andere KI-Tools? Ich prüfe Ihre Systeme auf:
- **Checklist:**
  - Betreiberpflichten nach EU AI Act
  - Haftungsrisiken (Human Oversight)
  - DSFA-Notwendigkeit (Art. 35 DSGVO)
- **Ergebnis:** Audit-Report + Prioritäten-Roadmap
- **Dauer:** 3-5 Arbeitstage
- **Preis:** Ab 3.000 € (Festpreis)
- **CTA:** Quickcheck buchen

**Karte 2: Sovereign AI Implementation**
- **Sub:** Für Premium-Privatpraxen, spezialisierte MVZs
- **Text:** Absolute Datensouveränität. Ihre KI verlässt niemals Ihre Hardware. Ich implementiere:
- **Checklist:**
  - Open-Source-Modelle (Llama, Mistral) lokal
  - Privacy-by-Design-Architektur
  - Volle Wahrung § 203 StGB (Schweigepflicht)
- **Ergebnis:** Ihr "digitales Fort Knox"
- **Dauer:** 4-8 Wochen (je nach Scope)
- **Preis:** Ab 15.000 € (Projekt)
- **CTA:** Erstgespräch vereinbaren

**Karte 3: Technical Compliance Engineering**
- **Sub:** Für KI-Anbieter, Health-Tech Startups
- **Text:** Ihre Entwickler bauen Features. Ich baue Compliance in den Code. Leistungen:
- **Checklist:**
  - Privacy-Proxies & Middleware-Entwicklung
  - MDR/AI Act-Zertifizierungsbegleitung
  - Compliance-as-Code (keine AGBs, echte Sicherheit)
- **Ergebnis:** Rechtssichere Architektur von Zeile 1
- **Dauer:** Projekt- oder Retainer-Basis
- **Preis:** Auf Anfrage (ab 2.000 €/Tag)
- **CTA:** Projektanfrage senden

---

## TrustAbout (Sektion "Über Mich")
**Headline:** Jurist. AI Engineer. Architekt.

**Text:**
Ich bin Daniel Kleiboldt. 10 Jahre Konzernjurist, spezialisiert auf Compliance an der Schnittstelle zwischen Regulatorik und Business.

Ich habe erlebt, wie Innovationen in Rechtsabteilungen scheitern – nicht weil sie illegal waren, sondern weil niemand die richtigen Fragen stellte.

Heute schließe ich diese Lücke. Als Legal Engineer verbinde ich tiefes juristisches Fachwissen mit technischer Umsetzungskompetenz. Ich baue Healthcare-KI, die von der ersten Codezeile an rechtssicher konzipiert ist.

**Slogan:** Privacy by Design. Nicht Privacy by Panik.

**Qualifikationen:**
- **Diplom-Jurist (WWU Münster):** Wirtschaftsrecht & Compliance
- **10 Jahre Corporate Law:** Vertragsgestaltung & Risk Management
- **AI Engineer:** Spezialisiert auf LLM-Architekturen & Privacy
- **Healthcare-Compliance:** EU AI Act, DSGVO Art. 9, MDR

---

## Insights (Artikel)
**Headline:** Klartext zu Recht & Technik.
**Sub:** Gedanken zu KI, DSGVO und dem Alltag in der Arztpraxis.

### Artikel 1: Ihre IT hat gestern ein Update installiert. Heute sind Sie Betreiber eines Hochrisiko-KI-Systems.
**Tag:** PRAXISSOFTWARE | **Datum:** FEB 2026
**Preview:** Donnerstagmorgen. Ihr IT-Dienstleister hat nachts routinemäßig Updates eingespielt. Alles läuft. Keine Störung. Kein Ticket im System. Was Sie nicht wissen: Mit diesem Update hat sich die rechtliche Einordnung Ihrer Praxissoftware fundamental geändert.

**Inhalt:**
Donnerstagmorgen. Ihr IT-Dienstleister hat nachts routinemäßig Updates eingespielt. Alles läuft. Keine Störung. Kein Ticket im System.

Was Sie nicht wissen: Mit diesem Update hat sich die rechtliche Einordnung Ihrer Praxissoftware fundamental geändert.

Gestern war Ihr Terminverwaltungssystem ein Kalendertool. Heute analysiert eine KI im Hintergrund Patienteneingaben, priorisiert automatisch Notfälle und schlägt basierend auf Symptombeschreibungen den passenden Facharzt vor.

Ihr Datenschutzkonzept beschreibt noch die alte Version. Ihre Auftragsverarbeitungsverträge decken die neuen Datenflüsse nicht ab. Ihre Ärzte wissen nicht, dass sie jetzt KI-generierte Entscheidungsvorschläge nutzen.

**Sie haften trotzdem.**

#### Das Problem: Wer kontrolliert die Updates?
Die zentrale Frage lautet: Wer bei Ihnen prüft, ob ein Software-Update die rechtliche Einordnung Ihres Systems verändert?

Wenn die Antwort "Niemand, das macht der Anbieter" lautet, haben wir ein Problem.

Der Software-Hersteller liefert Technik. Er trägt die Produkthaftung für sein Tool. Aber Sie entscheiden, dieses Tool in der Patientenversorgung einzusetzen. Sie sind der Betreiber. Und als Betreiber tragen Sie die Verantwortung dafür, dass der Einsatz rechtssicher erfolgt.

Der Hersteller kennt Ihre Prozesse nicht. Er weiß nicht, wie Ihre Ärzte die Software nutzen. Er kann nicht beurteilen, ob sein neues Feature in Ihrem spezifischen Kontext ein Compliance-Risiko erzeugt.

Das können nur Sie.

#### Drei konkrete Probleme

**1) Das DSFA-Dilemma**
Eine Datenschutz-Folgenabschätzung bildet die tatsächliche Datenverarbeitung ab. Wenn Ihre Software nach einem Update plötzlich Gesundheitsdaten an einen Sub-Prozessor zur KI-Analyse sendet oder automatisierte Priorisierungen vornimmt, beschreibt Ihre DSFA von 2023 nicht mehr die Realität.

Das ist kein Papierkram-Problem. Das ist ein Haftungsrisiko.

Wenn Ihr Landesdatenschutzbeauftragter prüft – und die prüfen inzwischen gezielt Healthcare-KI –, kann er feststellen: "Ihre dokumentierte Datenverarbeitung stimmt nicht mit der tatsächlichen überein."

Die Folge sind aufsichtsrechtliche Maßnahmen, die Ihren Betrieb empfindlich stören können. Bußgelder sind dabei oft noch das kleinere Übel im Vergleich zum Reputationsschaden.

Was ich in der Praxis sehe: Viele Betreiber gehen davon aus, dass der Software-Anbieter die DSFA aktualisiert. Das ist ein Missverständnis. Der Anbieter kann eine Muster-DSFA liefern. Aber die konkrete Folgenabschätzung für Ihren Betrieb müssen Sie durchführen.

**2) Heterogene IT-Landschaften (bei lokalen Installationen)**
Das größte Risiko in MVZs mit lokalen Servern oder zeitversetzten Rollouts: nicht synchronisierte IT-Systeme.

Standort A nutzt Software X in Version 3.2. Standort B hat Version 3.5, weil deren IT-Dienstleister schneller updated. Standort C läuft noch auf 3.1, weil ein kritisches Modul inkompatibel ist.

Jede Version hat andere Features. Unterschiedliche Datenflüsse. Unterschiedliche Risikoprofile. Ihre Compliance-Dokumentation geht aber von "der Software" aus.

Im Schadensfall müssen Sie beweisen können: "Wir wussten, welche Systeme wo liefen. Wir haben die Risiken bewertet." Wenn Sie das nicht können, deutet die fehlende Dokumentation darauf hin, dass Sie die Kontrolle nicht hatten.

(Hinweis: Bei reinen Cloud-Lösungen (SaaS) haben alle Nutzer meist die gleiche Version. Das Problem verschiebt sich dort auf die Frage: Wer prüft die Release Notes, bevor das Update automatisch ausgerollt wird?)

**3) Die schleichende Mutation der Zweckbestimmung**
Der EU AI Act unterscheidet KI-Systeme nach ihrer Zweckbestimmung. Ein reines Dokumentationswerkzeug unterliegt anderen Anforderungen als ein System, das klinische Entscheidungen beeinflusst.

Die Grenze ist messerscharf: Sobald Software nicht mehr nur dokumentiert, sondern Entscheidungen vorbereitet, ändert sich die Risikoklasse.

Ein Beispiel: Ihr Terminverwaltungssystem hat bisher Anrufe entgegengenommen und Termine nach Verfügbarkeit vergeben. Nach dem Update analysiert es Freitext-Eingaben ("starke Brustschmerzen seit heute Morgen"), erkennt potenzielle Notfälle und priorisiert diese automatisch.

Was sich technisch wie ein nützliches Feature anfühlt, ist juristisch eine Änderung der Zweckbestimmung. Aus Admin-Software wird ein Medizinprodukt.

Das Problem: Viele Software-Anbieter kommunizieren das nicht. Nicht aus Böswilligkeit – sondern weil sie selbst oft nicht wissen, wie ihr Feature rechtlich einzuordnen ist.

#### Was jetzt zu tun ist
Hier sind drei Schritte für ein pragmatisches Change-Management:

**1) Release-Review vor Rollout**
Bevor ein Major-Update eingespielt wird, prüft jemand – intern oder extern –, ob sich etwas Wesentliches ändert:
- Ändert sich die Zweckbestimmung?
- Neue Datenflüsse?
- Neue Automatismen?

Das muss keine vierstündige Analyse sein. In den meisten Fällen reicht ein strukturiertes 30-Minuten-Review der Release Notes.

**Wichtig: Fragen Sie Ihren Anbieter proaktiv nach den Release Notes, wenn diese nicht automatisch kommen.**

**2) DSFA-Update-Trigger**
Wenn eine der Fragen oben mit "Ja" beantwortet wird, löst das eine DSFA-Aktualisierung aus. Das ist deutlich weniger Aufwand als ein Bußgeldverfahren, in dem Sie nicht nachweisen können, dass Sie die Datenverarbeitung unter Kontrolle hatten.

**3) Dokumentierte Freigabe pro Standort**
Wer hat wann welches Update für welche Standorte freigegeben? Das muss nachvollziehbar sein. Eine einfache Tabelle reicht: Datum, Standort, Software-Version, freigebende Person, Bemerkungen.

#### Das eigentliche Problem
Das Grundproblem ist nicht technisch. Es ist organisatorisch.

IT-Sicherheit ist nicht dasselbe wie Compliance. Ihr IT-Dienstleister kann nicht beurteilen, ob ein Software-Update Ihre Datenschutzkonzepte obsolet macht oder Ihre Haftungsrisiken verändert.

Die Frage ist: Wessen Job ist es dann?

In vielen Organisationen lautet die ehrliche Antwort: "Niemandes." Es gibt eine Lücke zwischen IT-Betrieb und rechtlicher Verantwortung. Und diese Lücke wird größer, je komplexer Ihre IT-Landschaft wird.

#### Was das für Sie bedeutet
Wenn Sie eine heterogene IT-Landschaft über mehrere Standorte betreiben und sich gerade fragen, ob Ihre Compliance mit Ihren Updates Schritt hält – Sie sind nicht allein.

Sie brauchen einen Prozess. Einen schlanken, pragmatischen Governance-Mechanismus, der Software-Changes nicht blockiert, sondern rechtssicher macht.

Genau dafür gibt es Legal Engineering.

Wenn Sie wissen wollen, wie das konkret bei Ihnen aussehen könnte – lassen Sie uns reden. Bevor Ihr Datenschutzbeauftragter es tut.

#Praxissoftware #Update #Haftung #Compliance #LegalEngineering


### Artikel 2: Ihr KI-Anbieter weiß nicht, ob sein Produkt legal ist. (Und Sie haften trotzdem.)
**Tag:** Due Dilligence | **Datum:** JAN 2026
**Preview:** Dienstagvormittag. Verkaufsgespräch. Ein junger Mann in Sneakers und Hoodie sitzt Ihnen gegenüber. Sein Laptop glänzt mit Stickern von Tech-Konferenzen, die Sie nicht kennen.

**Inhalt:**
Dienstagvormittag. Verkaufsgespräch. Ein junger Mann in Sneakers und Hoodie sitzt Ihnen gegenüber. Sein Laptop glänzt mit Stickern von Tech-Konferenzen, die Sie nicht kennen.

Er zeigt Ihnen eine Demo. Eine KI, die Ihre Anrufe annimmt, Termine vergibt, Rezeptbestellungen entgegennimmt. Alles automatisch. Die Stimme klingt erstaunlich menschlich. Die MFA neben Ihnen sagt: "Das wäre ein Traum."

Sie fragen: "Wo genau liegen die Patientendaten während der Verarbeitung?"

Der junge Mann blinzelt. "Äh... in der Cloud?"

"Welche Cloud? Wo steht der Server?"

"Das muss ich nachschauen. Aber keine Sorge, wir sind voll DSGVO-konform."

Sie nicken höflich. Das Gespräch geht weiter. Zwei Wochen später unterschreiben Sie einen Zweijahresvertrag für €24.000.

Drei Monate später sitzt ein Brief vom Landesdatenschutzbeauftragten auf Ihrem Schreibtisch. Die gute Nachricht: Das KI-Tool funktioniert technisch einwandfrei. Die schlechte: Sie dürfen es nicht benutzen.

Das ist keine Dystopie. Das ist Alltag.

#### Die unbequeme Wahrheit über KI-Anbieter im Gesundheitswesen
Hier ist, was Ihnen niemand beim Verkaufsgespräch erzählt:

Die meisten KI-Anbieter, die gerade auf den Healthcare-Markt drängen, sind technisch brillant. Ihre Software funktioniert. Ihre Demos beeindrucken. Ihre Versprechen klingen verlockend.

Aber viele von ihnen haben ihre juristische Hausaufgaben nicht gemacht.

Nicht aus Bosheit. Sondern weil sie aus einer Welt kommen, in der "DSGVO-konform" ein Buzzword auf der Landing Page ist. Kein Konzept. Keine Architektur-Entscheidung. Kein Design-Prinzip.

In anderen Branchen mag das funktionieren. In Healthcare nicht.

Und hier kommt der Punkt, an dem es unangenehm wird: Wenn etwas schiefgeht, haften nicht die. Sie haften Sie.

#### Warum "DSGVO-konform" auf der Website nichts bedeutet
Sie kennen das vom Autokauf. Verkäufer verspricht: "Unfallfrei, Originalzustand, TÜV-frisch." Dann schauen Sie in die Papiere. Oder lassen es jemanden anschauen, der sich auskennt. Und plötzlich stellt sich heraus: Wurde mal seitlich touchiert, Motor läuft unrund, TÜV läuft in zwei Monaten ab.

Bei Software ist es dasselbe. Nur dass niemand unter die Motorhaube schaut.

"DSGVO-konform" ist das neue "klimaneutral". Jeder schreibt es auf die Website. Die wenigsten können erklären, was das konkret bedeutet. Und noch weniger können es beweisen.

Hier ist, was in vielen Fällen dahintersteckt:
- Ein Anwalt hat die Datenschutzerklärung geschrieben. (Gut.)
- Niemand hat geprüft, ob die tatsächliche Software macht, was die Datenschutzerklärung verspricht. (Weniger gut.)
- Der Server steht irgendwo. Keiner weiß genau wo. Aber "Cloud" klingt modern. (Katastrophe.)

Das Problem ist nicht, dass die Anbieter böse sind. Das Problem ist, dass sie nicht aus Healthcare kommen.

In der normalen Tech-Welt ist Datenschutz ein Compliance-Thema. Etwas, das man abhakt, bevor man launcht. In Healthcare ist es Betriebsgrundlage.

Und das bedeutet: Sie müssen die Fragen stellen, die der Anbieter nicht von sich aus beantwortet. Weil er sie entweder nicht kennt. Oder nicht beantworten kann.

#### Die vier Fragen, die Ihr Anbieter beantworten können muss (und oft nicht kann)
Wenn Sie das nächste Mal in einem KI-Verkaufsgespräch sitzen, stellen Sie diese vier Fragen. In dieser Reihenfolge. Und beobachten Sie die Reaktion.

**Frage 1: "Wo genau liegen meine Patientendaten während der Verarbeitung?"**
Die Antwort "in der Cloud" ist keine Antwort. Das ist wie "irgendwo in Europa".

Was Sie wissen müssen:
- Steht der Server in der EU? (Wenn nicht: rote Flagge.)
- Welcher Anbieter hostet? AWS, Google Cloud, Microsoft Azure? (Legitim, aber dann brauchen Sie Details.)
- Werden Daten in Drittländer übertragen? (USA ist kompliziert. Andere Länder ohne Angemessenheitsbeschluss: vergessen Sie es.)

Die richtige Antwort klingt so: "Wir nutzen Server in Frankfurt, gehostet bei diesem konkreten Anbieter, Daten verlassen nie die EU. Hier ist unser Datenschutzkonzept."

Die falsche Antwort klingt so: "Das muss ich nachschauen. Aber keine Sorge, ist alles sicher."

Wenn Ihr Anbieter nicht in 30 Sekunden erklären kann, wo Ihre Patientendaten landen, ist das Gespräch vorbei. Höflich verabschieden, Tür zu.

**Frage 2: "Wer haftet, wenn Ihre KI einen Fehler macht?"**
Die unbequeme Wahrheit: Sie. Nicht der Anbieter.

Das hat das Landgericht Kiel 2024 sehr klar gesagt: Wer ein Werkzeug nutzt, haftet für das Ergebnis. Die KI ist ein Werkzeug. Wenn das Skalpell abrutscht, können Sie auch nicht den Hersteller verklagen. Aber – und das ist wichtig – Ihr Anbieter muss Ihnen die Möglichkeit geben, die KI zu kontrollieren.

Das bedeutet konkret:
- Kann ich jeden Output der KI prüfen, bevor er rausgeht? (Wenn nicht: rote Flagge.)
- Gibt es eine "Auto-Send"-Funktion? (Wenn ja: noch größere rote Flagge.)
- Kann ich nachvollziehen, was die KI gemacht hat? (Audit-Logs, Versionierung, Dokumentation.)

Die richtige Antwort klingt so: "Alle KI-Outputs müssen von einem Menschen freigegeben werden. Wir loggen jeden Schritt. Hier ist unser Konzept für Human Oversight."

Die falsche Antwort klingt so: "Unsere KI ist sehr zuverlässig, da passiert nichts."

Spoiler: Doch, passiert. Und wenn es passiert, stehen Sie vor Gericht. Nicht der Typ im Hoodie.

**Frage 3: "Haben Sie ein Datenschutzkonzept nach EU AI Act?"**
Seit August 2024 gilt der AI Act. KI im Gesundheitsbereich ist Hochrisiko-Kategorie. Das bedeutet: Dokumentationspflichten, Konformitätsbewertungen, menschliche Aufsicht. Ab dem 2. August 2026 wird das voll durchgesetzt.

Wenn Ihr Anbieter jetzt sagt: "Machen wir bis dahin", übersetzen Sie das mental mit: "Haben wir noch nicht." Und fragen Sie sich: Will ich meine Praxis auf ein Versprechen bauen?

Die richtige Antwort klingt so: "Wir haben ein AI-Act-Compliance-Konzept. Unsere Zweckbestimmung ist die Unterstützung der medizinischen Dokumentation ohne Diagnosevorschläge. Unsere Risikoklasse ist Hochrisiko nach Anhang III. Hier sind die Unterlagen."

Die falsche Antwort klingt so: "Wir sind voll compliant." (Ohne Details. Ohne Unterlagen. Nur Buzzwords.)

Noch eine Variante der falschen Antwort: "Der AI Act betrifft uns nicht." (Spoiler: Doch. Tut er.)

**Frage 4: "Wie dokumentiere ich, dass ich die KI-Outputs geprüft habe?"**
Das ist die Frage, die kaum jemand stellt. Aber sie ist die wichtigste. Denn im Haftungsfall reicht es nicht zu sagen: "Ich hab's geprüft." Sie müssen es beweisen.

Das bedeutet:
- Gibt es Logs, die zeigen, wann ich welches Dokument freigegeben habe?
- Kann ich nachweisen, dass mein Team im Umgang mit der KI geschult wurde?
- Kann ich dokumentieren, dass ich die Grenzen der KI kenne?

Die richtige Antwort klingt so: "Jede Freigabe wird geloggt. Wir bieten Schulungen an. Hier ist unser Dokumentationsvorlage für Ihre Praxis."

Die falsche Antwort klingt so: "Müssen Sie nicht dokumentieren, ist ja nur ein Tool."

Falsch. Müssen Sie. Und wenn der Anbieter das nicht weiß, weiß er auch sonst nicht viel.

#### Was die Antworten über Ihren Anbieter verraten
Hier ist die Wahrheit: Die meisten KI-Anbieter sind keine Betrüger. Sie sind Optimisten. Sie glauben fest daran, dass ihr Produkt brillant ist. Und oft stimmt das sogar. Technisch.

Aber viele von ihnen haben nie in einer Arztpraxis gearbeitet. Sie wissen nicht, was passiert, wenn der Datenschutzbeauftragte anruft. Sie haben noch nie einen Haftungsfall erlebt. Sie denken, Compliance ist ein Häkchen auf der To-Do-Liste. Bis es das nicht mehr ist. Und dann stehen nicht die vor Gericht. Sondern Sie.

Deshalb: Stellen Sie die vier Fragen oben. Nicht, weil Sie misstrauisch sein müssen. Sondern weil Sie Ihre Praxis schützen müssen. Ein guter Anbieter wird Ihnen dankbar sein für die Fragen. Weil er weiß, dass Sie verstanden haben, worum es geht. Ein schlechter Anbieter wird ausweichen. Oder mit Buzzwords antworten.

Ihre Aufgabe ist es nicht, das Produkt zu verstehen. Ihre Aufgabe ist es zu verstehen, ob Sie es nutzen dürfen. Und ob Sie im Ernstfall beweisen können, dass Sie sorgfältig gehandelt haben.

#### Der eigentliche Produktivitätsgewinn
Hier ist die Ironie: Die KI, die wirklich Zeit spart, ist nicht die mit der beeindruckendsten Demo. Es ist die, bei der Sie nachts ruhig schlafen können. Weil Sie wissen: Die Daten liegen richtig. Die Haftung ist geklärt. Die Dokumentation steht. Wenn etwas schiefgeht – und irgendwann geht immer etwas schief –, können Sie beweisen, dass Sie alles richtig gemacht haben.

Das ist der wahre Produktivitätsgewinn. Nicht zwei Stunden mehr am Tag. Sondern null Stunden vor Gericht. KI in der Praxis ist keine Frage von "ob". Es ist eine Frage von "wie". Und "wie" beginnt mit den richtigen Fragen.

#### Was Sie jetzt tun können
**Wenn Sie gerade KI evaluieren:** Drucken Sie sich die vier Fragen aus. Nehmen Sie sie mit ins nächste Verkaufsgespräch. Stellen Sie sie. Alle vier. Und beobachten Sie, was passiert.

**Wenn Sie schon KI nutzen:** Stellen Sie die Fragen Ihrem Anbieter. Jetzt. Nicht morgen. Nicht nächsten Monat. Jetzt. Wenn er sie nicht beantworten kann, haben Sie ein Problem. Aber besser, Sie wissen es jetzt als in zwei Jahren, wenn der Brief vom Anwalt kommt.

**Wenn Sie unsicher sind:** Lassen Sie es jemanden anschauen, der beide Seiten versteht. Technik und Recht. Produktivität und Compliance. Begeisterung und Vorsicht. Das kostet ein paar Stunden Beratung. Das ist deutlich günstiger als ein Bußgeld. Oder ein Haftungsfall. Oder ein Produkt, das Sie nach zwei Jahren nicht mehr nutzen dürfen.

#### Das Fazit
KI ist keine Magie. Sie ist Werkzeug. Und wie bei jedem Werkzeug gilt: Wer es nutzt, muss wissen, was es kann. Und was es nicht kann. Und wer haftet, wenn es schiefgeht. Ihr Anbieter verkauft Ihnen das Werkzeug. Aber die Verantwortung verkauft er nicht mit. Die behalten Sie. Also stellen Sie die Fragen. Bevor Sie unterschreiben.

#LegalEngineering #KI #Arztpraxis #DSGVO #Compliance


### Artikel 3: Die Arzthelferin kündigt. Schon wieder. Kann KI helfen? (Kommt drauf an.)
**Tag:** Fachkräftemangel | **Datum:** DEZ 2025
**Preview:** Montagmorgen, 8:02 Uhr. Das Telefon klingelt. Niemand nimmt ab, weil die eine MFA, die heute da ist, gerade einem Patienten erklärt, dass seine Überweisung abgelaufen ist.

**Inhalt:**
Montagmorgen, 8:02 Uhr. Das Telefon klingelt. Niemand nimmt ab, weil die eine MFA, die heute da ist, gerade einem Patienten erklärt, dass seine Überweisung abgelaufen ist. Im Wartezimmer wächst die Schlange. Um 8:15 Uhr ist der Tag bereits verloren.

Diese Szene kennt jeder, der schon mal in einer deutschen Arztpraxis war – auf beiden Seiten des Tresens.

Und jetzt kommt der Punkt, an dem normalerweise jemand sagt: „KI kann das lösen!" Spoiler: Kann sie nicht. Zumindest nicht so, wie es die LinkedIn-Posts der Tech-Bros versprechen.

Aber – und das ist der interessante Teil – sie kann etwas anderes tun. Etwas, das tatsächlich hilft. Wenn man es richtig macht. Was selten passiert.

#### Reden wir kurz über den Elefanten im Wartezimmer: Fachkräftemangel.
Jede dritte Arztpraxis findet keine MFAs mehr. In manchen Regionen ist es jede zweite. Das liegt nicht daran, dass der Beruf so unattraktiv wäre – okay, doch, auch daran – sondern an einer simplen demografischen Rechnung: mehr Patienten, weniger Arbeitskräfte, keine Besserung in Sicht.

Die Standardlösung heißt: „Wir müssen attraktiver werden, besser bezahlen, mehr ausbilden." Stimmt alles. Wird aber auch nicht reichen.

Denn das Problem ist nicht nur die Zahl der Leute. Es ist das, womit sie ihre Zeit verbringen.

Eine MFA in einer durchschnittlichen Praxis verbringt – je nach Studie – 40 bis 60 Prozent ihrer Arbeitszeit mit Dingen, die nichts mit Patienten zu tun haben. Telefon. Formulare. Überweisungen suchen. Befunde einscannen. Termine jonglieren. Der ganze administrative Wahnsinn, der zwischen „Patient kommt rein" und „Patient geht raus" passiert.

Das ist die unsichtbare Zeitfresser-Maschine. Und genau hier wird KI interessant.

#### Ich rede nicht von Science-Fiction. Ich rede von Technologie, die jetzt existiert und im Einsatz ist:
- **Telefonassistenz:** Eine KI nimmt Anrufe an, versteht natürliche Sprache, beantwortet die ewig gleichen Fragen (Öffnungszeiten, Rezeptbestellung, „Hat der Doktor noch einen Termin frei?") und leitet nur die komplizierten Fälle an einen Menschen weiter. Ergebnis: 60 bis 80 Prozent weniger Unterbrechungen für die MFA. Das klingt nach Marketing-Versprechen, ist aber nachvollziehbar, wenn man sich anschaut, wie repetitiv die meisten Praxis-Anrufe sind.
- **Digitale Patientenaufnahme:** Der Patient füllt den Anamnesebogen vor dem Termin auf dem Handy aus. Die KI strukturiert die Angaben, erkennt Muster, bereitet eine Zusammenfassung vor. Keine Klemmbrett-Zettel mehr, die jemand entziffern und abtippen muss. Klingt banal, spart aber 10 bis 15 Minuten pro Neupatient – und die Daten sind lesbar.
- **Dokumentationsunterstützung:** Die KI hört beim Arzt-Patienten-Gespräch zu, erstellt automatisch eine strukturierte Dokumentation, schlägt ICD-Codes vor. Der Arzt prüft, korrigiert, bestätigt. Statt nach Feierabend noch eine Stunde Diktate abzutippen.

Klingt gut, oder? Ist es auch. Theoretisch.

#### Und jetzt der Teil, den einem keiner erzählt:
Hier wird es juristisch – aber bleiben Sie dran, das ist der wichtige Teil.

Gesundheitsdaten sind keine normalen Daten. Sie fallen unter Artikel 9 DSGVO, „besondere Kategorien personenbezogener Daten". Die Verarbeitung ist grundsätzlich verboten. Mit Ausnahmen, ja – aber diese Ausnahmen sind eng. Sehr eng.

Und seit August 2024 gilt der EU AI Act. KI im Gesundheitsbereich gilt als Hochrisiko-Kategorie. Dokumentationspflichten, Konformitätsbewertungen, menschliche Aufsicht. Das volle Programm.

#### Was heißt das praktisch?
- **Einwilligung ist nicht „Häkchen setzen".** Der Patient muss verstehen, was passiert. Dass eine KI zuhört. Dass seine Daten verarbeitet werden. Von wem, wo, wie lange. „Ich hab die AGB akzeptiert" reicht nicht.
- **Serverstandort ist kein Technik-Detail.** Viele KI-Tools laufen über amerikanische Cloud-Dienste. Nach dem Schrems-II-Urteil ist Datentransfer in die USA ein juristisches Minenfeld. Fragen Sie mal Ihren KI-Anbieter, wo genau die Daten verarbeitet werden. Die Antwort ist oft: betretenes Schweigen.
- **Black-Box-KI ist ein Haftungsproblem.** Wenn die KI etwas dokumentiert oder empfiehlt, müssen Sie erklären können, wie sie zu diesem Ergebnis kam. „Der Algorithmus hat das gesagt" ist keine Dokumentation, die vor der Ärztekammer oder im Haftungsfall Bestand hat.

Die meisten KI-Lösungen, die gerade auf den Markt drängen, sind technisch brilliant und rechtlich ein Himmelfahrtskommando. Nicht, weil KI schlecht ist. Sondern weil die Entwickler das Produkt gebaut haben – und dann erst gefragt haben, ob es legal ist.

Das ist kein Vorwurf. So funktioniert Tech-Entwicklung oft. Erst bauen, dann fragen.

Im Gesundheitsbereich ist das nur leider keine Option. Hier kann „erst bauen, dann fragen" bedeuten: Bußgeld vom Landesdatenschutzbeauftragten. Oder ein Patient, der klagt. Oder eine Ärztekammer, die Fragen stellt.

#### Aber – und das ist wichtig – KI in der Arztpraxis kann funktionieren.
Rechtssicher, sinnvoll, ohne Albträume.

Das funktioniert allerdings nur, wenn man aufhört, es als reines Tech-Projekt zu behandeln. Wenn Compliance nicht nachträglich draufgeklebt wird, sondern von Anfang an mitgedacht. Privacy by Design statt Privacy by Panik.

Das bedeutet: Bevor Sie investieren, klären Sie, wo die Daten liegen. Wie die Einwilligung eingeholt wird. Was passiert, wenn die KI Unsinn macht. Wer haftet.

Das klingt nach Aufwand. Ist es auch. Aber es ist deutlich weniger Aufwand als der Brief vom Anwalt, nachdem Sie schon 50.000 Euro in eine Lösung investiert haben, die Sie nicht nutzen dürfen.

Wenn Sie eine Praxis gründen, umbauen oder erweitern – jetzt ist der Moment, das von Anfang an richtig zu machen. Greenfield-Projekte sind selten. Nutzen Sie die Chance.

Und wenn Sie eine KI-Lösung für den Gesundheitsbereich anbieten: Sprechen Sie mit jemandem, der beide Seiten versteht. Bevor Ihre Kunden es tun müssen.

#KI #Gesundheitswesen #DSGVO #Fachkräftemangel #LegalEngineering


### Artikel 4: Der Arztbrief lügt. Wer zahlt? (Spoiler: Nicht die KI.)
**Tag:** HAFTUNG | **Datum:** Nov 2025
**Preview:** Freitagnachmittag, 16:30 Uhr. Der Kopf raucht, das Wartezimmer leert sich endlich. Das neue KI-Tool hat den Arztbrief für Herrn Müller schon fertiggeschrieben. Ein kurzer Blick, digital signiert, raus damit. Feierabend.

**Inhalt:**
Freitagnachmittag, 16:30 Uhr. Der Kopf raucht, das Wartezimmer leert sich endlich.

Das neue KI-Tool hat den Arztbrief für Herrn Müller schon fertiggeschrieben. Ein kurzer Blick, digital signiert, raus damit. Feierabend.

Zwei Wochen später liegt Post auf dem Tisch. Nicht von Herrn Müller, sondern von seinem Anwalt. Ein Befund wurde falsch übertragen, die Weiterbehandlung war fehlerhaft.

Und jetzt kommt der Punkt, an dem viele Ärzte denken: „Dafür haftet doch der Software-Hersteller, oder?“

**Spoiler: Nein. Tut er nicht.**

Das ist der Moment, in dem aus einem digitalen Komfort-Feature ein existenzbedrohendes Risiko wird.

#### Reden wir kurz über die Illusion der „intelligenten“ Verantwortung.
Wir sind es gewohnt, dass Technologie funktioniert. Wenn das MRT kaputt ist, rufen wir den Techniker. Wenn die KI halluziniert (also Dinge erfindet), rufen wir niemanden – wir merken es oft gar nicht.

Das Landgericht Kiel hat im November 2024 das ausgesprochen, was Juristen schon lange predigen: Wer KI einsetzt, haftet für das Ergebnis. Ohne Wenn und Aber.

Das Urteil stellt klar: Die Plausibilitätsprüfung ist nicht delegierbar. Nicht an die Software. Nicht an den Hersteller. Und auch nicht an die MFA, die „nur mal schnell drüberschauen“ soll.

Das klingt hart. Ist es auch. Aber es ist konsequent.

KI ist im juristischen Sinne kein Kollege, den man um Rat fragt. Sie ist ein Werkzeug. Wie ein Skalpell. Wenn das Skalpell abrutscht, können Sie auch nicht sagen: „Der Stahl war schuld.“ Sie führen die Hand.

#### Was heißt das für die Praxis?
Das bedeutet nicht, dass Sie KI wieder abschaffen müssen (das wäre bei dem aktuellen Personalmangel auch Wahnsinn). Es bedeutet aber, dass sich Ihr Workflow ändern muss.

Der Satz „Die KI hat das so geschrieben“ ist vor Gericht keine Verteidigung. Er ist ein Schuldeingeständnis. Er beweist nämlich, dass Sie ein Werkzeug benutzt haben, dessen Grenzen Sie nicht verstanden haben.

#### Hier sind die drei Dinge, die Sie tun müssen, damit Sie nachts ruhig schlafen können:
**1. Human Oversight (Der menschliche Veto-Knopf)**
Jeder, wirklich jeder Output einer KI braucht einen menschlichen Prüfschritt. Und zwar nicht pro forma („Häkchen setzen“), sondern inhaltlich. Sie müssen den Arztbrief lesen, bevor Sie ihn signieren. Klingt nach Arbeit? Ja. Aber es geht immer noch schneller, als ihn selbst zu tippen. Der Zeitgewinn liegt im Entwurf, nicht in der Endkontrolle.

**2. Dokumentation ist Ihre Lebensversicherung**
Wenn etwas schiefgeht, müssen Sie beweisen können, dass Sie sorgfältig gearbeitet haben. Audit-Logs, Prüfprotokolle, Nachweise darüber, dass Sie und Ihr Team im Umgang mit den Grenzen der KI geschult wurden. Ohne Dokumentation ist es im Haftungsfall so, als hätten Sie nie geprüft.

**3. Verträge lesen (Ja, wirklich)**
Schauen Sie in die AGB Ihres KI-Anbieters. Viele schließen jede Haftung für inhaltliche Fehler aus. Das ist branchenüblich, aber für Sie ein Warnsignal. Wenn der Anbieter keine Verantwortung übernimmt, müssen Ihre internen Sicherheitsnetze umso engmaschiger sein.

#### Das Fazit:
KI in der Praxis ist ein kalkulierbares Risiko – keine unkontrollierbare Gefahr.

Gefährlich wird es nur, wenn man „Automatisierung“ mit „blindem Vertrauen“ verwechselt. Wer die KI als das behandelt, was sie ist – ein extrem leistungsfähiger, aber manchmal verwirrter Assistent –, der gewinnt Zeit. Wer sie als Orakel behandelt, verliert im Zweifel seine Zulassung.

**Nutzen Sie die Technologie. Aber behalten Sie die Verantwortung.**

#KI #Haftung #Arztpraxis #DigitalHealth #LegalEngineering


### Artikel 5: Ihr Software-Vertrag läuft bis 2026? Dann haben wir ein Thema.
**Tag:** AI ACT | **Datum:** Okt 2025
**Preview:** Ab August 2026 gelten neue Regeln für KI in der Medizin. Was müssen Praxen wissen, und wann wird Software zum Hochrisiko-System?

**Inhalt:**
2. August 2026. Das klingt nach ferner Zukunft. Für den Praxisalltag ist das eine Ewigkeit. Aber für Ihre IT-Strategie ist es morgen.

An diesem Stichtag greift der EU AI Act voll durch. Und wenn Sie heute Software einkaufen oder Lizenzverträge verlängern, die über dieses Datum hinauslaufen, sollten Sie jetzt genau hinschauen.

Denn viele Tools, die heute noch als "innovative Assistenzsysteme" verkauft werden, wachen 2026 in einer neuen Realität auf: Sie sind plötzlich regulierte Hochrisiko-Systeme.

#### Was das Gesetz eigentlich will
Der AI Act teilt KI nicht nach Intelligenz ein, sondern nach Risiko. Und Medizin ist – wenig überraschend – fast immer Hochrisiko. Das bedeutet für die Hersteller: Massive Anforderungen an Transparenz, Dokumentation und menschliche Aufsicht.

Aber hier liegt das Detail, das Ihnen kaum ein Vertriebler erzählt: Nicht jede KI in der Praxis ist automatisch betroffen. Es kommt nicht auf die Technik an. Es kommt auf den Zweck an.

#### Die entscheidende Frage: Schreibt sie nur, oder denkt sie schon?
Die Grenze ist dünn, aber juristisch messerscharf.

Ein Beispiel: Ein Diktiersystem, das Ihre gesprochenen Worte in Text umwandelt, ist ein Werkzeug. Es dokumentiert. Risiko: Niedrig. Ein System, das aus Ihren Notizen mitliest und automatisch Diagnosevorschläge oder Therapieoptionen generiert, ist eine Entscheidungshilfe. Risiko: Hoch.

Die Grenze verläuft genau dort, wo die Software aufhört, Sekretär zu sein, und anfängt, Assistenzarzt zu spielen. Sobald klinische Entscheidungen beeinflusst werden, greifen die strengen Regeln.

#### Was Sie jetzt tun müssen
Wenn Sie gerade dabei sind, Ihre Praxis zu digitalisieren, stellen Sie Ihrem Anbieter eine simple Frage: "Wie ist die Zweckbestimmung Ihrer Software nach dem AI Act definiert?"

Es gibt zwei mögliche Reaktionen.

Die gute: Der Anbieter legt Ihnen eine Roadmap vor, wie er bis 2026 die Konformität sicherstellt. Die schlechte (und häufige): Betretenes Schweigen.

**Spoiler: Wenn Ihr Anbieter nicht weiß, ob sein Produkt in 18 Monaten noch legal betreibbar ist, sollten Sie keinen 24-Monats-Vertrag unterschreiben.**

Der AI Act ist keine bürokratische Schikane, um Ärzte zu ärgern. Er ist eine Chance zur Professionalisierung. Er zwingt den Markt, die "Black Box" zu öffnen.

Wer jetzt plant, hat 18 Monate Vorsprung. Das ist genug Zeit, um die Spreu vom Weizen zu trennen. Nutzen Sie sie.

#AIAct #Compliance #DigitalHealth #Medizinrecht #LegalEngineering

---

## FAQ (Häufige Fragen)
**Headline:** Antworten auf Ihre Fragen.

**1. Mein KI-Anbieter sagt, alles ist DSGVO-konform. Reicht das nicht?**
Nein. "DSGVO-konform" auf einer Marketing-Seite ist eine Zusicherung des Herstellers – nicht Ihre Absicherung als Betreiber.
Der Hersteller liefert Software. Sie setzen sie ein. Und Sie haften, wenn etwas schiefgeht.
Beispiel: Doctolib hat C5-Testat und ISO-Zertifizierungen. Trotzdem müssen SIE als Praxis:
- Datenschutz-Folgenabschätzung durchführen
- Human Oversight technisch umsetzen
- Patienteninformation rechtskonform gestalten
Ich prüfe, was der Anbieter liefert – und was Sie selbst tun müssen.

**2. Kann ich nicht einfach einen Anwalt fragen?**
Klar. Aber der sagt Ihnen, ob etwas legal ist – nicht, wie Sie es technisch umsetzen.
Klassisches Beispiel:
- Jurist: "Sie brauchen Human Oversight."
- Sie: "Okay... und wie bau ich das?"
- Jurist: "Keine Ahnung, fragen Sie Ihren IT-Dienstleister."
- IT-Dienstleister: "Keine Ahnung, fragen Sie einen Juristen."
Ich bin beides. Ich sage nicht nur "was", sondern auch "wie".

**3. Was kostet mich das?**
Kommt drauf an, was Sie brauchen:
- Quickcheck (Audit): Ab 3.000 € (Festpreis)
- Sovereign AI (Implementierung): Ab 15.000 € (Projekt)
- Retainer (laufende Beratung): Ab 2.000 €/Monat
Vergleich: Eine falsche Compliance-Entscheidung kostet Sie entweder:
- 50.000 € (Bußgeld Datenschutzbeauftragter)
- 100.000+ € (Haftungsfall)
- Ihre Zulassung (Ärztekammer)
Ich bin günstiger als jedes dieser Szenarien.

**4. Warum sollte ich Ihnen vertrauen?**
Gute Frage. Drei Gründe:
1. Ich habe 10 Jahre Corporate Compliance gemacht. Ich weiß, wie Haftung funktioniert.
2. Ich verstehe KI technisch. Ich kann Code lesen und Architektur beurteilen.
3. Ich bin kein Verkäufer. Wenn Ihre Lösung rechtssicher ist, sage ich das. Wenn nicht, sage ich das auch.
Referenzen auf Anfrage.

**5. Ist das nicht alles übertrieben? Bisher ist doch nichts passiert.**
Antwort: Noch nicht.
Der EU AI Act gilt seit August 2024. Die Hochrisiko-Anforderungen greifen ab August 2026.
Wir sind in der Übergangsphase. Aktuell passiert wenig – aber das ändert sich.
Vergleich: DSGVO galt ab Mai 2018. Die ersten großen Bußgelder kamen 2019/2020. Wer vorher compliance war, hatte Ruhe. Der Rest zahlte.
Gleiches Muster beim AI Act.

---

## Contact (Kontakt)
**Headline:** Der erste Schritt: Klarheit statt Haftungsrisiko.
**Sub:** Sie planen KI-Einsatz in Ihrer Praxis? Oder nutzen bereits Tools wie Doctolib?

**Angebot:**
Im kostenlosen 20-Minuten-Gespräch klären wir:
- Wo liegen Ihre Haftungsrisiken?
- Welche Betreiberpflichten gelten für Sie?
- Wie setzen Sie das technisch um?
Kein Verkaufsgespräch. Nur Klarheit.

**Buttons:**
- Erstgespräch buchen
- mail@kleiboldt.de
- LinkedIn

---

## Footer
**Links:**
- Angebot: KI-Compliance-Audit, Sovereign AI Roadmap, Technical Engineering
- Kontakt: mail@kleiboldt.de, LinkedIn, Erstgespräch buchen
- Rechtliches: Impressum, Datenschutz

**Copyright:** © 2025 Daniel Kleiboldt. Made in Germany.
**Badges:** DSGVO Konform, SSL Secured

---

## Impressum & Datenschutz (Modals)

### Impressum
**Angaben gemäß § 5 TMG**
Daniel Kleiboldt
Büro für Legal Engineering
Hülsbrockstraße 115
33334 Gütersloh

**Kontakt**
Telefon: 05241/7082012
E-Mail: mail@kleiboldt.de

**Redaktionell verantwortlich**
Verantwortlich für den Inhalt nach § 18 Abs. 2 MStV:
Daniel Kleiboldt
Hülsbrockstraße 115
33334 Gütersloh

**EU-Streitschlichtung**
Die Europäische Kommission stellt eine Plattform zur Online-Streitbeilegung (OS) bereit: https://ec.europa.eu/consumers/odr/
Unsere E-Mail-Adresse finden Sie oben im Impressum.

**Verbraucherstreitbeilegung/Universalschlichtungsstelle**
Ich bin nicht bereit oder verpflichtet, an Streitbeilegungsverfahren vor einer Verbraucherschlichtungsstelle teilzunehmen.

### Datenschutz
**1. Datenschutz auf einen Blick**
Die folgenden Hinweise geben einen einfachen Überblick darüber, was mit Ihren personenbezogenen Daten passiert, wenn Sie diese Website besuchen.

**2. Hosting**
Wir hosten unsere Website bei Vercel Inc., USA.

**3. Tools und Dienste**
Wir nutzen Vercel Web Analytics (Privacy-First) und Calendly für die Terminbuchung.
